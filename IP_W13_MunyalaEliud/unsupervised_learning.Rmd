---
title: "Supervised Learning in R"
author: "Munyala Eliud"
date: "Nov 26, 2021"
output: html_document
---

# Problem Statement


# ----------------installs and libraries--------------------------
```{r}

library(tidyverse)
library(readr) # enable us to read csv files
library(dplyr)
#install.packages('caTools')
library(caTools)
library(caret)
#install.packages('caret')
#Install if the package doesn't exist 
#install.packages('DataExplorer) 
#library(DataExplorer)

```

# Load Data
```{r}

df_kira_plast <- read.csv(url('http://bit.ly/EcommerceCustomersDataset'))

```


```{r}
head(df_kira_plast)
```

```{r}
view(df_kira_plast)
```

```{r}
tail(df_kira_plast)
```

```{r}
#check columns
names(df_kira_plast)

```

```{r}
is.null(df_kira_plast)
```


# ------ EXPLORATORY DATA ANALYSIS -----------------

> The students find and deal with outliers, anomalies, and missing data within the dataset. 

```{r}
summary(df_kira_plast)
```

```{r}
# check data types
str(df_kira_plast)
```


# ----------- Univariate,Bivariate,Multivariate -------------

```{r}
# frequency distribution table
ggplot(df_kira_plast,aes(x=Month)) + geom_bar()

```

```{r}

# frequency distribution table
ggplot(df_kira_plast,aes(x=VisitorType)) + geom_bar()

```

```{r}
# frequency distribution table
ggplot(df_kira_plast,aes(x=Revenue)) + geom_bar()
```

```{r}
# frequency distribution table
ggplot(df_kira_plast,aes(x=Weekend)) + geom_bar()
```


# ---------------- Classification ------------------

# 

```{r}
str(df_kira_plast)
```


```{r}
df_new_1 <- df_kira_plast[,c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates"     ,"ExitRates","PageValues","SpecialDay","OperatingSystems","Browser","Region","TrafficType","VisitorType")]

```


```{r}
str(df_new_1)
```

```{r}


# Normalizing the dataset so that no particular attribute 
# has more impact on clustering algorithm than others.
# ---
# 
normalize <- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}

for (i in colnames(df_new_1[,1:14])){
  normalize(df_new_1[[i]])
}
```


```{r}

```


```{r}
result <- kmeans(df_new_1,3)
```

```{r}
result$centers
```

```{r}
result$cluster
```

```{r}
head(df_new_1)
```

```{r}

str(df_new_1)
```

```{r}
sum(is.na(df_new_1))
```

```{r}
#remove NA values from vector
df_new_1 <- na.omit(df_new_1)
```

```{r}
df_new_1
```

# feature engineering

```{r}
result <- kmeans(df_new_1,3)
```



```{r}

# Normalizing the dataset so that no particular attribute 
# has more impact on clustering algorithm than others.
# ---
# 
normalize <- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}



```

```{r}

for (i in colnames(df_new_1)){
  df_new_1[[i]] <- normalize(df_new_1[[i]])
}
```



```{r}
# We check the dimensions of out training dataframe and testing dataframe
# ---
# 
dim(training); 
dim(testing);
```



```{r}

```



# ------------- Supervised learning with SVM ------------------
```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```


```{r}
svm_Linear <- train(VisitorType ~., data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
```

```{r}
# ---
# 
svm_Linear
```

```{r}
# 
test_pred <- predict(svm_Linear, newdata = testing)
test_pred
```

```{r}
# Now checking for our accuracy of our model by using a confusion matrix 
# ---
# 
confusionMatrix(table(test_pred, testing$VisitorType))
```



